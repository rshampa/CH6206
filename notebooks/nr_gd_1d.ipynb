{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization methods for non-linear functions in 1-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Importing required python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions using python's \"def\" keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define function, its first derivative, Hessian\n",
    "def f_1(x):\n",
    "    return 3*x**3 - 10*x**2 - 56*x + 5\n",
    "\n",
    "def Grad_f_1(x):\n",
    "    return 3*3*x**2 - 2*10*x - 56\n",
    "\n",
    "def Hessian_f_1(x):\n",
    "    return 2*3**3*x - 2*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Range of x-values\n",
    "x = np.linspace(-4,6,100)\n",
    "##Plotting f_1(x)\n",
    "plt.plot(x, f_1(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Newton-Raphson (NR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NR function - inputs: Grad (gradient), Hess (Hessian), x-value, epsilon (precision),\n",
    "##nMAx (maximum number of iterations)\n",
    "def Newton_Raphson_Optimize(Grad, Hess, x, epsilon=1.0, nMax = 1000):\n",
    "    ##Initialization: i (iteration counter), iter_x used for plots\n",
    "    i = 0\n",
    "    iter_x, iter_count = np.empty(0), np.empty(0)\n",
    "    error = 10.0\n",
    "    X = np.array([x])\n",
    "    \n",
    "    #Looping as long as error is greater than epsilon\n",
    "    while np.linalg.norm(error) > epsilon and i < nMax:\n",
    "        ##Variables for plotting\n",
    "        i +=1\n",
    "        iter_x = np.append(iter_x,x)\n",
    "        iter_count = np.append(iter_count,i)   \n",
    "        #print(X, f_1(X)) \n",
    "        \n",
    "        ##Calculate new values\n",
    "        X_prev = X\n",
    "        X = X -  ( (1/Hess(x)) * Grad(x))\n",
    "        error = X - X_prev\n",
    "        x = X[0]\n",
    "        \n",
    "    #print(f_1(X))     \n",
    "    return X, iter_x, iter_count\n",
    "\n",
    "\n",
    "#root, iter_x, iter_count = Newton_Raphson_Optimize(Grad_f_1, Hessian_f_1, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GD function - inputs: Grad (gradient), x-value, gamma (step-size), epsilon (precision),\n",
    "##nMAx (maximum number of iterations)\n",
    "def Gradient_Descent(Grad, x, gamma = 0.01, epsilon=1.0, nMax = 1000 ):\n",
    "    #Initialization\n",
    "    i = 0\n",
    "    iter_x, iter_count = np.empty(0), np.empty(0)\n",
    "    error = 10\n",
    "    X = np.array([x])\n",
    "    \n",
    "    #Looping as long as error is greater than epsilon\n",
    "    while np.linalg.norm(error) > epsilon and i < nMax:\n",
    "        ##Variables for plotting\n",
    "        i +=1\n",
    "        iter_x = np.append(iter_x,x)\n",
    "        iter_count = np.append(iter_count ,i)   \n",
    "        #print(X,f_1(X))\n",
    "        \n",
    "        ##Calculate new values\n",
    "        X_prev = X\n",
    "        X = X - gamma * Grad(x)\n",
    "        error = X - X_prev\n",
    "        x = X[0]\n",
    "          \n",
    "    #print(f_1(X))\n",
    "    return X, iter_x, iter_count\n",
    "\n",
    "\n",
    "#root, iter_x, iter_count = Gradient_Descent(Grad_f_1, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing NR and GD optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_gd, iter_x_gd, iter_count_gd = Gradient_Descent(Grad_f_1, 3, 0.001, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_nr, iter_x_nr, iter_count_nr = Newton_Raphson_Optimize(Grad_f_1, Hessian_f_1, 3, 0.001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f_1(x), c = \"r\")\n",
    "plt.scatter(iter_x_gd, f_1(iter_x_gd), color = 'g', marker = '*')\n",
    "plt.scatter(iter_x_nr, f_1(iter_x_nr), color = 'b', marker = 'o')\n",
    "plt.gca().legend(('f(x)','GD','NR'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method rapidly reaches maxima/minima; though attracts to saddle points; it's a drawback of this method; also depends on starting position; expensive as it needs to compute Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
